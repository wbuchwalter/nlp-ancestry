(this["webpackJsonprd3t-demo"]=this["webpackJsonprd3t-demo"]||[]).push([[0],{104:function(e,t,a){},108:function(e,t,a){},109:function(e,t,a){},110:function(e,t,a){},116:function(e,t,a){"use strict";a.r(t);var n=a(0),s=a.n(n),r=a(7),i=a.n(r),o=a(51),l=a(26),c=a(27),d=a(37),p=a(32),u=a(29),m=a(62),h=a.n(m),g=a(138),f=a(140),b=a(65),k=a.n(b),v=a(66),E=a.n(v),N=a(139),w=(a(104),a(47)),C=function(e){Object(p.a)(a,e);var t=Object(u.a)(a);function a(){var e;return Object(l.a)(this,a),(e=t.call(this)).state={foreignObjectProps:{width:400,height:1e3,x:-420,y:-200}},e}return Object(c.a)(a,[{key:"render",value:function(){var e=w[this.props.modelName];return s.a.createElement("div",{className:"ModelCard"},s.a.createElement("div",{className:"CardHeader"},s.a.createElement("h2",{className:"ModelName"},this.props.modelName),e.hfLink&&s.a.createElement(N.a,{title:"View on HuggingFace"},s.a.createElement(f.a,{className:"HeaderButton",onClick:function(t){return window.open(e.hfLink,"_blank")}},s.a.createElement("img",{className:"HfIcon",src:"/nlp-ancestry/huggingface_logo.svg"}))),e.paperLink&&s.a.createElement(N.a,{title:"Open model's paper"},s.a.createElement(f.a,{className:"HeaderButton",onClick:function(t){return window.open(e.paperLink,"_blank")}},s.a.createElement(k.a,{className:"ButtonIcon"}))),e.resultsLink&&s.a.createElement(N.a,{title:"Open model's results"},s.a.createElement(f.a,{className:"HeaderButton",onClick:function(t){return window.open(e.resultsLink,"_blank")}},s.a.createElement(E.a,{className:"ButtonIcon",alt:"???"})))),s.a.createElement(g.a,null),e.contribs&&s.a.createElement("div",null,s.a.createElement("h3",null,"Main Contributions"),s.a.createElement("ul",null,e.contribs.map((function(e){return s.a.createElement("li",{key:e},e)})))),e.uses&&s.a.createElement("div",null,s.a.createElement("h3",null,"Primary Uses"),s.a.createElement("ul",null,e.uses.map((function(e){return s.a.createElement("li",{key:e},e)})))))}}]),a}(n.Component),L=(a(108),{vertical:{title:{textAnchor:"start",x:40},attributes:{},attribute:{x:40,dy:"1.2em"}},horizontal:{title:{textAnchor:"start",y:40},attributes:{x:0,y:40},attribute:{x:0,dy:"1.2em"}}}),x=function(e){Object(p.a)(a,e);var t=Object(u.a)(a);function a(){var e;return Object(l.a)(this,a),(e=t.call(this)).state={showModelCard:!1},e}return Object(c.a)(a,[{key:"showPopUp",value:function(){this.props.nodeClick(this.props.nodeDatum.name)}},{key:"render",value:function(){var e=this;return s.a.createElement(s.a.Fragment,null,s.a.createElement("g",null,s.a.createElement("text",Object.assign({className:"rd3t-label__title"},L[this.props.orientation].title,{onClick:this.props.onNodeClick}),this.props.nodeDatum.name)),this.state.showModelCard&&s.a.createElement(C,{modelInfo:this.props.nodeDatum}),s.a.createElement("circle",{className:"circle",r:20,onClick:function(t){return e.showPopUp()}}))}}]),a}(n.Component),y=(a(109),a(67)),M=function(e){Object(p.a)(a,e);var t=Object(u.a)(a);function a(){var e;return Object(l.a)(this,a),(e=t.call(this)).handleNodeClick=function(){},e.addedNodesCount=0,e.nodeFn=function(t,a){return s.a.createElement(x,{nodeDatum:t.nodeDatum,toggleNode:t.toggleNode,orientation:a.orientation,nodeClick:e.showModelCard})},e.state={showModelCard:!1,selectedModelName:null,data:y,orientation:"vertical",translateX:200,translateY:300,collapsible:!0,shouldCollapseNeighborNodes:!1,initialDepth:999,depthFactor:void 0,zoomable:!0,zoom:1,scaleExtent:{min:.1,max:1},separation:{siblings:1,nonSiblings:1},nodeSize:{x:200,y:200},enableLegacyTransitions:!1,transitionDuration:500,renderCustomNodeElement:e.nodeFn},e.setTreeData=e.setTreeData.bind(Object(d.a)(e)),e.showModelCard=e.showModelCard.bind(Object(d.a)(e)),e}return Object(c.a)(a,[{key:"setTreeData",value:function(e){this.setState({data:e})}},{key:"showModelCard",value:function(e){this.setState(Object(o.a)(Object(o.a)({},this.state),{},{showModelCard:!this.state.showModelCard,selectedModelName:e}))}},{key:"componentDidMount",value:function(){var e=this.treeContainer.getBoundingClientRect();this.setState({translateX:e.width/1.75,translateY:200})}},{key:"render",value:function(){var e=this;w["GPT-2"];return s.a.createElement("div",{className:"App"},s.a.createElement("div",{className:"demo-container"},s.a.createElement("div",{className:"column-right"},s.a.createElement("div",{ref:function(t){return e.treeContainer=t},className:"tree-container"},s.a.createElement(h.a,{data:this.state.data,renderCustomNodeElement:this.state.renderCustomNodeElement?function(t){return e.state.renderCustomNodeElement(t,e.state)}:void 0,rootNodeClassName:"node",leafNodeClassName:"node",branchNodeClassName:"node",orientation:this.state.orientation,translate:{x:this.state.translateX,y:this.state.translateY},pathFunc:this.state.pathFunc,collapsible:this.state.collapsible,initialDepth:this.state.initialDepth,zoomable:this.state.zoomable,zoom:this.state.zoom,scaleExtent:this.state.scaleExtent,nodeSize:this.state.nodeSize,separation:this.state.separation,enableLegacyTransitions:this.state.enableLegacyTransitions,transitionDuration:this.state.transitionDuration,depthFactor:this.state.depthFactor,shouldCollapseNeighborNodes:this.state.shouldCollapseNeighborNodes})),this.state.showModelCard&&s.a.createElement("div",{className:"modal"},s.a.createElement(C,{modelName:this.state.selectedModelName})))))}}]),a}(n.Component);a(110);i.a.render(s.a.createElement(M,null),document.getElementById("root"))},47:function(e){e.exports=JSON.parse('{"Transformers":{"contribs":["First proposed the transformer architecture","Demonstrated the effectiveness of attention-only models"],"paperLink":"https://arxiv.org/pdf/1706.03762v5.pdf","resultsLink":"https://paperswithcode.com/paper/attention-is-all-you-need#results"},"GPT":{"contribs":["Demonstrated generative pre-training on unlabeled text"],"uses":["Writing assistance","Creative writing and art","Entertrainment"],"tags":["NLG"],"paperLink":"https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf","resultsLink":"https://paperswithcode.com/paper/improving-language-understanding-by#results"},"GPT-2":{"contribs":["First model to be scaled to 1.5 billion parameters","Pretrained on a large raw text corpus using Language Modeling objective"],"uses":["Writing assistance (completion, summarization etc.)","Creative writing and art","Entertrainment"],"tags":["NLG"],"hfLink":"https://huggingface.co/gpt2","paperLink":"https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf","resultsLink":"https://paperswithcode.com/paper/language-models-are-unsupervised-multitask#results"},"GPT-3":{"contribs":["First model to be scaled to 175 billion parameters","Demonstrated effective few-shot abilities"],"uses":["Writing assistance (completion, summarization etc.)","Creative writing and art","Entertrainment"],"tags":["NLG"],"paperLink":"https://arxiv.org/pdf/2005.14165.pdf","resultsLink":"https://paperswithcode.com/paper/language-models-are-few-shot-learners#results"},"BERT":{"contribs":["Extremely easy to fine-tune","Introduced masked language modeling training objective"],"uses":["Next sentence prediction","Masked language modeling","Can be fine-tuned on a downstream task."],"hfLink":"https://huggingface.co/bert-base-uncased","paperLink":"https://arxiv.org/pdf/1810.04805v2.pdf","resultsLink":"https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional#results"},"DistilBERT":{"hfLink":"https://huggingface.co/distilbert-base-uncased","paperLink":"https://arxiv.org/pdf/1910.01108v4.pdf","resultsLink":"https://paperswithcode.com/paper/distilbert-a-distilled-version-of-bert#results"},"RoBERTa":{"hfLink":"https://huggingface.co/roberta-base","paperLink":"https://arxiv.org/pdf/1907.11692v1.pdf","resultsLink":"https://paperswithcode.com/paper/roberta-a-robustly-optimized-bert-pretraining#results"},"XLM":{"paperLink":"https://arxiv.org/pdf/1901.07291v1.pdf","resultsLink":"https://paperswithcode.com/paper/cross-lingual-language-model-pretraining#results"},"XLM-R":{"hfLink":"https://huggingface.co/xlm-roberta-base","paperLink":"https://arxiv.org/abs/1911.02116"},"T5":{"hfLink":"https://huggingface.co/t5-base","paperLink":"https://arxiv.org/pdf/1910.10683v3.pdf","resultsLink":"https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning#results"}}')},67:function(e){e.exports=JSON.parse('{"name":"Transformers","children":[{"name":"GPT","children":[{"name":"GPT-2","children":[{"name":"GPT-3"}]}]},{"name":"BERT","children":[{"name":"DistilBERT"},{"name":"RoBERTa"},{"name":"XLM","children":[{"name":"XLM-R"}]}]},{"name":"T5"}]}')},74:function(e,t,a){e.exports=a(116)}},[[74,1,2]]]);
//# sourceMappingURL=main.fafd9834.chunk.js.map